---
title: Kimi-Linear
date: "2025-10-31T18:28:51.740Z"
tags:
  - event
  - ai
---

[Kimi-Linear](https://arxiv.org/pdf/2510.26692) 是符合直觉的一个工程优化（所以大概率会在未来的其它开源模型上普遍使用）。

k2的上下文不是256K吗，这次变成1M，之所以\*4是因为它只做了1/4的全局注意力，所以运行成本相对来说涨幅就没那么剧烈。

---

传统全注意力机制是，一个新的token得和之前所有的token全部计算一遍。所以成本是平方增长。
因为训练有很多层，它就保留1/4的层（MLA）还继续做这种全注意力机制。
剩下的3/4的层（KDA）就做固定大小的计算。

---

这3/4的层毕竟是要“遗忘”的，因为大小固定。
它其实就是一种类似人类短期注意力的能力。

我自己觉得论文最有价值的就是这部分。

---

这种混合的架构，只要好好做，就是会比全注意力的架构效果更好。
一个是因为全注意力的层减少了，同时配合KDA做了一些协同优化，减少的工作量，因此过拟合的问题被削减了。
第二是因为它模拟了人类的短期记忆，所以更像人类了：因为人类记忆力不足，所以看到信息，就要赶紧泛化成一个概念。抽象成理念后有一套通用的方法论来解决问题。
因此人类世界里面的各种发明：代码、文字、各种工具，用起来会更契合，更像人。

---

我估计未来这种模块可以和更多模块混合搭配在一起，比如和稀疏注意力的模块搭配在一起训练，代码能力可能还能再提升。

还有不同的领域专家训练的时候做不同的比例混搭，这都是有可能的。

---

但这种技术很符合直觉，本来就不是Kimi的专属技术。学习新东西的前提是遗忘。因此这里的关键在于“如何遗忘”。
一开始是梯度下降（DeltaNet），很死板，适合一些非常原始的智能；
然后升级成，根据内容生成一个比例去衰减（Gated DeltaNet）；
到KIMI这里，是对信息分类成多通道，为不同的通道去配置独立的遗忘比例。

未来可能还能加入更多的外部信息维度，来进一步影响各个通道的遗忘配比。总之就是维度越多元，就能越像人类的智能。但是收益肯定是边际递减的。
